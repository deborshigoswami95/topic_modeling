{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcbff83",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation - MCMC implementation\n",
    "\n",
    "## Sources\n",
    "\n",
    "1. This notebook directly follows the on the same topic by Andrew Brooks. [link](http://brooksandrew.github.io/simpleblog/articles/latent-dirichlet-allocation-under-the-hood/)\n",
    "2. The Andrew Brooks tutorial strongly follows some of the original works of topic modeling by [Griffith et. al.](https://webfiles.uci.edu/msteyver/publications/Griffiths_Steyvers_Tenenbaum_2007.pdf)\n",
    "\n",
    "## Details\n",
    "\n",
    "While the original tutorial is in R, this code implements the same algorithm in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b702f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5642ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_topic_model(docs:list,num_topics:int):\n",
    "    \n",
    "    docs=[i.split() for i in docs]\n",
    "    word_map = {word:idx for idx,word in enumerate(list(set([i for j in docs for i in j])))}\n",
    "    docs=[[word_map[i] for i in j] for j in docs]\n",
    "    inv_word_map={k:v for v,k in word_map.items()}\n",
    "    \n",
    "    word_topic=np.zeros(shape=(num_topics,len(word_map)))\n",
    "    topic_assignment=[[0 for i in j] for j in docs]\n",
    "    #topic_assignment=[np.array(i) for i in np.array(topic_assignment,dtype=object)]\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "        for j in range(len(docs[i])):\n",
    "            topic_assignment[i][j]=np.random.randint(low=0,high=num_topics)\n",
    "            topic_index=topic_assignment[i][j]\n",
    "            word_index=docs[i][j]\n",
    "            word_topic[topic_index,word_index]+=1\n",
    "    doc_topic_count=np.zeros(shape=(len(docs),num_topics))\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "        for j in range(num_topics):\n",
    "            doc_topic_count[i,j]=len([k for k in topic_assignment[i] if k==j])\n",
    "            \n",
    "    return docs,word_map,inv_word_map,word_topic,topic_assignment,doc_topic_count\n",
    "\n",
    "\n",
    "def fit_topic_model(docs:list,num_topics:int,num_iterations:int,eta:float,alpha:float):\n",
    "    \n",
    "    docs,word_map,inv_word_map,word_topic,\\\n",
    "    topic_assignment,doc_topic_count = initialize_topic_model(docs,num_topics)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        for d in range(len(docs)):\n",
    "            for w in range(len(docs[d])):\n",
    "\n",
    "                topic_prev=topic_assignment[d][w]\n",
    "                word_id=docs[d][w]\n",
    "\n",
    "                doc_topic_count[d,topic_prev]-=1\n",
    "                word_topic[topic_prev,word_id]-=1\n",
    "\n",
    "                # Update topic assignment\n",
    "                denom_a = len(docs[d]) + num_topics * alpha\n",
    "                denom_b = word_topic.sum(axis=1) + len(word_map) * eta\n",
    "\n",
    "                update_probability=np.divide((word_topic[:,word_id] + eta),denom_b) \n",
    "                update_probability+= (doc_topic_count[d,:] + alpha)/denom_a\n",
    "                update_probability=update_probability/sum(update_probability)\n",
    "\n",
    "                # sample a word based on new update probability distribution\n",
    "                topic_new=np.random.choice(list(range(num_topics)),1,p=update_probability/sum(update_probability))[0]\n",
    "\n",
    "                topic_assignment[d][w]=topic_new\n",
    "                doc_topic_count[d,topic_new]+=1\n",
    "                word_topic[topic_new,word_id]+=1\n",
    "\n",
    "                if topic_prev!=topic_new:\n",
    "                    print('doc: {}, token: {}\\t\\t\\ttopic {} => {}'.format(d,\n",
    "                                                                          inv_word_map[word_id],\n",
    "                                                                          topic_prev,\n",
    "                                                                          topic_new))\n",
    "    return doc_topic_count, word_topic, topic_assignment, inv_word_map\n",
    "                \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f6ec852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 0, token: turkey\t\t\ttopic 1 => 2\n",
      "doc: 0, token: day\t\t\ttopic 2 => 1\n",
      "doc: 0, token: holiday\t\t\ttopic 0 => 1\n",
      "doc: 1, token: i\t\t\ttopic 2 => 0\n",
      "doc: 1, token: to\t\t\ttopic 1 => 2\n",
      "doc: 1, token: on\t\t\ttopic 1 => 2\n",
      "doc: 1, token: holiday\t\t\ttopic 0 => 1\n",
      "doc: 2, token: turkey\t\t\ttopic 2 => 1\n",
      "doc: 2, token: trot\t\t\ttopic 0 => 1\n",
      "doc: 2, token: on\t\t\ttopic 1 => 2\n",
      "doc: 2, token: thanksgiving\t\t\ttopic 2 => 1\n",
      "doc: 3, token: race\t\t\ttopic 2 => 0\n",
      "doc: 4, token: time\t\t\ttopic 0 => 2\n",
      "doc: 4, token: travel\t\t\ttopic 0 => 2\n",
      "doc: 4, token: space\t\t\ttopic 0 => 2\n",
      "doc: 5, token: on\t\t\ttopic 2 => 0\n",
      "doc: 5, token: thanksgiving\t\t\ttopic 1 => 0\n",
      "doc: 6, token: space\t\t\ttopic 1 => 2\n",
      "doc: 6, token: museum\t\t\ttopic 1 => 0\n",
      "doc: 6, token: cool\t\t\ttopic 2 => 0\n",
      "doc: 7, token: aspiring\t\t\ttopic 1 => 0\n",
      "doc: 7, token: star\t\t\ttopic 0 => 2\n",
      "doc: 0, token: eat\t\t\ttopic 1 => 0\n",
      "doc: 0, token: turkey\t\t\ttopic 2 => 1\n",
      "doc: 0, token: turkey\t\t\ttopic 2 => 1\n",
      "doc: 0, token: holiday\t\t\ttopic 1 => 2\n",
      "doc: 1, token: i\t\t\ttopic 0 => 2\n",
      "doc: 1, token: cake\t\t\ttopic 2 => 1\n",
      "doc: 1, token: holiday\t\t\ttopic 1 => 0\n",
      "doc: 2, token: turkey\t\t\ttopic 1 => 2\n",
      "doc: 2, token: race\t\t\ttopic 2 => 1\n",
      "doc: 4, token: time\t\t\ttopic 2 => 1\n",
      "doc: 4, token: travel\t\t\ttopic 2 => 1\n",
      "doc: 4, token: race\t\t\ttopic 2 => 1\n",
      "doc: 5, token: movie\t\t\ttopic 0 => 2\n",
      "doc: 6, token: movie\t\t\ttopic 1 => 2\n",
      "doc: 6, token: at\t\t\ttopic 0 => 1\n",
      "doc: 6, token: air\t\t\ttopic 0 => 1\n",
      "doc: 6, token: museum\t\t\ttopic 0 => 1\n",
      "doc: 6, token: cool\t\t\ttopic 0 => 1\n",
      "doc: 6, token: movie\t\t\ttopic 0 => 2\n",
      "doc: 7, token: star\t\t\ttopic 2 => 0\n",
      "doc: 0, token: eat\t\t\ttopic 0 => 1\n",
      "doc: 0, token: turkey\t\t\ttopic 1 => 2\n",
      "doc: 0, token: on\t\t\ttopic 1 => 0\n",
      "doc: 0, token: turkey\t\t\ttopic 1 => 2\n",
      "doc: 0, token: holiday\t\t\ttopic 2 => 1\n",
      "doc: 1, token: like\t\t\ttopic 1 => 2\n",
      "doc: 1, token: cake\t\t\ttopic 1 => 2\n",
      "doc: 1, token: on\t\t\ttopic 2 => 1\n",
      "doc: 1, token: holiday\t\t\ttopic 0 => 2\n",
      "doc: 2, token: trot\t\t\ttopic 1 => 2\n",
      "doc: 2, token: race\t\t\ttopic 1 => 2\n",
      "doc: 2, token: on\t\t\ttopic 2 => 1\n",
      "doc: 2, token: holiday\t\t\ttopic 1 => 2\n",
      "doc: 3, token: the\t\t\ttopic 0 => 1\n",
      "doc: 3, token: turtle\t\t\ttopic 0 => 1\n",
      "doc: 4, token: time\t\t\ttopic 1 => 2\n",
      "doc: 4, token: space\t\t\ttopic 2 => 1\n",
      "doc: 5, token: movie\t\t\ttopic 2 => 0\n",
      "doc: 5, token: on\t\t\ttopic 0 => 2\n",
      "doc: 6, token: movie\t\t\ttopic 2 => 0\n",
      "doc: 6, token: air\t\t\ttopic 1 => 0\n",
      "doc: 6, token: and\t\t\ttopic 0 => 1\n",
      "doc: 6, token: space\t\t\ttopic 2 => 0\n",
      "doc: 6, token: is\t\t\ttopic 2 => 0\n",
      "doc: 6, token: movie\t\t\ttopic 2 => 0\n",
      "doc: 7, token: star\t\t\ttopic 0 => 2\n",
      "doc: 0, token: eat\t\t\ttopic 1 => 2\n",
      "doc: 0, token: turkey\t\t\ttopic 2 => 1\n",
      "doc: 0, token: on\t\t\ttopic 0 => 1\n",
      "doc: 0, token: turkey\t\t\ttopic 2 => 1\n",
      "doc: 1, token: i\t\t\ttopic 2 => 1\n",
      "doc: 1, token: like\t\t\ttopic 2 => 1\n",
      "doc: 1, token: to\t\t\ttopic 2 => 1\n",
      "doc: 1, token: cake\t\t\ttopic 2 => 1\n",
      "doc: 1, token: on\t\t\ttopic 1 => 2\n",
      "doc: 1, token: holiday\t\t\ttopic 2 => 1\n",
      "doc: 2, token: turkey\t\t\ttopic 2 => 1\n",
      "doc: 2, token: race\t\t\ttopic 2 => 1\n",
      "doc: 2, token: on\t\t\ttopic 1 => 2\n",
      "doc: 2, token: thanksgiving\t\t\ttopic 1 => 2\n",
      "doc: 2, token: holiday\t\t\ttopic 2 => 0\n",
      "doc: 3, token: snail\t\t\ttopic 0 => 1\n",
      "doc: 3, token: race\t\t\ttopic 0 => 2\n",
      "doc: 4, token: time\t\t\ttopic 2 => 0\n",
      "doc: 4, token: travel\t\t\ttopic 1 => 2\n",
      "doc: 4, token: space\t\t\ttopic 1 => 0\n",
      "doc: 4, token: race\t\t\ttopic 1 => 2\n",
      "doc: 5, token: on\t\t\ttopic 2 => 0\n",
      "doc: 6, token: at\t\t\ttopic 1 => 0\n",
      "doc: 6, token: and\t\t\ttopic 1 => 2\n",
      "doc: 6, token: cool\t\t\ttopic 1 => 0\n",
      "doc: 7, token: movie\t\t\ttopic 0 => 1\n",
      "doc: 7, token: star\t\t\ttopic 2 => 1\n",
      "doc: 0, token: turkey\t\t\ttopic 1 => 0\n",
      "doc: 0, token: on\t\t\ttopic 1 => 2\n",
      "doc: 0, token: turkey\t\t\ttopic 1 => 2\n",
      "doc: 0, token: day\t\t\ttopic 1 => 2\n",
      "doc: 0, token: holiday\t\t\ttopic 1 => 0\n",
      "doc: 1, token: i\t\t\ttopic 1 => 0\n",
      "doc: 1, token: like\t\t\ttopic 1 => 2\n",
      "doc: 1, token: to\t\t\ttopic 1 => 2\n",
      "doc: 1, token: eat\t\t\ttopic 1 => 2\n",
      "doc: 1, token: on\t\t\ttopic 2 => 0\n",
      "doc: 1, token: holiday\t\t\ttopic 1 => 0\n",
      "doc: 2, token: race\t\t\ttopic 1 => 2\n",
      "doc: 2, token: on\t\t\ttopic 2 => 0\n",
      "doc: 2, token: thanksgiving\t\t\ttopic 2 => 0\n",
      "doc: 2, token: holiday\t\t\ttopic 0 => 2\n",
      "doc: 3, token: race\t\t\ttopic 2 => 0\n",
      "doc: 3, token: the\t\t\ttopic 1 => 0\n",
      "doc: 4, token: time\t\t\ttopic 0 => 2\n",
      "doc: 4, token: space\t\t\ttopic 0 => 2\n",
      "doc: 5, token: thanksgiving\t\t\ttopic 0 => 2\n",
      "doc: 6, token: air\t\t\ttopic 0 => 1\n",
      "doc: 6, token: and\t\t\ttopic 2 => 0\n",
      "doc: 6, token: museum\t\t\ttopic 1 => 0\n",
      "doc: 7, token: star\t\t\ttopic 1 => 0\n",
      "doc: 0, token: eat\t\t\ttopic 2 => 0\n",
      "doc: 0, token: turkey\t\t\ttopic 0 => 1\n",
      "doc: 0, token: turkey\t\t\ttopic 2 => 1\n",
      "doc: 0, token: day\t\t\ttopic 2 => 0\n",
      "doc: 0, token: holiday\t\t\ttopic 0 => 2\n",
      "doc: 1, token: i\t\t\ttopic 0 => 2\n",
      "doc: 1, token: like\t\t\ttopic 2 => 1\n",
      "doc: 1, token: to\t\t\ttopic 2 => 0\n",
      "doc: 1, token: cake\t\t\ttopic 1 => 0\n",
      "doc: 1, token: holiday\t\t\ttopic 0 => 2\n",
      "doc: 2, token: trot\t\t\ttopic 2 => 1\n",
      "doc: 2, token: race\t\t\ttopic 2 => 1\n",
      "doc: 2, token: on\t\t\ttopic 0 => 2\n",
      "doc: 2, token: thanksgiving\t\t\ttopic 0 => 2\n",
      "doc: 3, token: snail\t\t\ttopic 1 => 0\n",
      "doc: 3, token: race\t\t\ttopic 0 => 1\n",
      "doc: 3, token: the\t\t\ttopic 0 => 2\n",
      "doc: 3, token: turtle\t\t\ttopic 1 => 0\n",
      "doc: 4, token: travel\t\t\ttopic 2 => 0\n",
      "doc: 6, token: air\t\t\ttopic 1 => 0\n",
      "doc: 6, token: space\t\t\ttopic 0 => 2\n",
      "doc: 7, token: aspiring\t\t\ttopic 0 => 1\n",
      "doc: 7, token: movie\t\t\ttopic 1 => 2\n",
      "doc: 7, token: star\t\t\ttopic 0 => 1\n",
      "doc: 0, token: turkey\t\t\ttopic 1 => 2\n",
      "doc: 0, token: on\t\t\ttopic 2 => 0\n",
      "doc: 0, token: turkey\t\t\ttopic 1 => 2\n",
      "doc: 1, token: i\t\t\ttopic 2 => 0\n",
      "doc: 1, token: like\t\t\ttopic 1 => 2\n",
      "doc: 1, token: cake\t\t\ttopic 0 => 1\n",
      "doc: 1, token: on\t\t\ttopic 0 => 2\n",
      "doc: 2, token: turkey\t\t\ttopic 1 => 0\n",
      "doc: 2, token: holiday\t\t\ttopic 2 => 0\n",
      "doc: 3, token: snail\t\t\ttopic 0 => 2\n",
      "doc: 3, token: the\t\t\ttopic 2 => 0\n",
      "doc: 3, token: turtle\t\t\ttopic 0 => 1\n",
      "doc: 4, token: time\t\t\ttopic 2 => 0\n",
      "doc: 4, token: travel\t\t\ttopic 0 => 2\n",
      "doc: 4, token: space\t\t\ttopic 2 => 0\n",
      "doc: 4, token: race\t\t\ttopic 2 => 1\n",
      "doc: 5, token: on\t\t\ttopic 0 => 2\n",
      "doc: 5, token: thanksgiving\t\t\ttopic 2 => 0\n",
      "doc: 6, token: movie\t\t\ttopic 0 => 2\n",
      "doc: 6, token: at\t\t\ttopic 0 => 1\n",
      "doc: 6, token: space\t\t\ttopic 2 => 0\n",
      "doc: 6, token: cool\t\t\ttopic 0 => 1\n",
      "doc: 7, token: movie\t\t\ttopic 2 => 1\n",
      "doc: 0, token: eat\t\t\ttopic 0 => 1\n",
      "doc: 0, token: turkey\t\t\ttopic 2 => 0\n",
      "doc: 0, token: on\t\t\ttopic 0 => 2\n",
      "doc: 0, token: turkey\t\t\ttopic 2 => 0\n",
      "doc: 0, token: day\t\t\ttopic 0 => 1\n",
      "doc: 0, token: holiday\t\t\ttopic 2 => 1\n",
      "doc: 1, token: to\t\t\ttopic 0 => 2\n",
      "doc: 1, token: eat\t\t\ttopic 2 => 0\n",
      "doc: 1, token: cake\t\t\ttopic 1 => 0\n",
      "doc: 1, token: holiday\t\t\ttopic 2 => 0\n",
      "doc: 2, token: turkey\t\t\ttopic 0 => 1\n",
      "doc: 2, token: trot\t\t\ttopic 1 => 0\n",
      "doc: 2, token: race\t\t\ttopic 1 => 0\n",
      "doc: 2, token: on\t\t\ttopic 2 => 0\n",
      "doc: 2, token: thanksgiving\t\t\ttopic 2 => 0\n",
      "doc: 2, token: holiday\t\t\ttopic 0 => 1\n",
      "doc: 3, token: snail\t\t\ttopic 2 => 1\n",
      "doc: 3, token: the\t\t\ttopic 0 => 1\n",
      "doc: 4, token: travel\t\t\ttopic 2 => 0\n",
      "doc: 4, token: space\t\t\ttopic 0 => 1\n",
      "doc: 4, token: race\t\t\ttopic 1 => 0\n",
      "doc: 5, token: movie\t\t\ttopic 0 => 2\n",
      "doc: 5, token: on\t\t\ttopic 2 => 1\n",
      "doc: 5, token: thanksgiving\t\t\ttopic 0 => 1\n",
      "doc: 6, token: at\t\t\ttopic 1 => 2\n",
      "doc: 6, token: and\t\t\ttopic 0 => 2\n",
      "doc: 6, token: space\t\t\ttopic 0 => 2\n",
      "doc: 6, token: is\t\t\ttopic 0 => 2\n",
      "doc: 6, token: cool\t\t\ttopic 1 => 2\n",
      "doc: 7, token: aspiring\t\t\ttopic 1 => 2\n",
      "doc: 7, token: movie\t\t\ttopic 1 => 2\n",
      "doc: 0, token: eat\t\t\ttopic 1 => 2\n",
      "doc: 0, token: turkey\t\t\ttopic 0 => 1\n",
      "doc: 0, token: on\t\t\ttopic 2 => 0\n",
      "doc: 0, token: day\t\t\ttopic 1 => 2\n",
      "doc: 0, token: holiday\t\t\ttopic 1 => 0\n",
      "doc: 1, token: like\t\t\ttopic 2 => 0\n",
      "doc: 1, token: to\t\t\ttopic 2 => 0\n",
      "doc: 1, token: eat\t\t\ttopic 0 => 2\n",
      "doc: 1, token: on\t\t\ttopic 2 => 1\n",
      "doc: 1, token: holiday\t\t\ttopic 0 => 1\n",
      "doc: 2, token: turkey\t\t\ttopic 1 => 2\n",
      "doc: 2, token: race\t\t\ttopic 0 => 2\n",
      "doc: 2, token: on\t\t\ttopic 0 => 2\n",
      "doc: 3, token: race\t\t\ttopic 1 => 2\n",
      "doc: 3, token: the\t\t\ttopic 1 => 0\n",
      "doc: 3, token: turtle\t\t\ttopic 1 => 0\n",
      "doc: 4, token: time\t\t\ttopic 0 => 2\n",
      "doc: 4, token: travel\t\t\ttopic 0 => 1\n",
      "doc: 4, token: space\t\t\ttopic 1 => 0\n",
      "doc: 4, token: race\t\t\ttopic 0 => 2\n",
      "doc: 5, token: movie\t\t\ttopic 2 => 0\n",
      "doc: 5, token: thanksgiving\t\t\ttopic 1 => 0\n",
      "doc: 6, token: movie\t\t\ttopic 2 => 1\n",
      "doc: 6, token: air\t\t\ttopic 0 => 2\n",
      "doc: 6, token: and\t\t\ttopic 2 => 0\n",
      "doc: 6, token: museum\t\t\ttopic 0 => 1\n",
      "doc: 6, token: movie\t\t\ttopic 0 => 2\n",
      "doc: 7, token: movie\t\t\ttopic 2 => 1\n",
      "doc: 7, token: star\t\t\ttopic 1 => 2\n",
      "doc: 0, token: eat\t\t\ttopic 2 => 0\n",
      "doc: 0, token: turkey\t\t\ttopic 1 => 0\n",
      "doc: 0, token: day\t\t\ttopic 2 => 1\n",
      "doc: 0, token: holiday\t\t\ttopic 0 => 1\n",
      "doc: 1, token: i\t\t\ttopic 0 => 1\n",
      "doc: 1, token: eat\t\t\ttopic 2 => 0\n",
      "doc: 1, token: on\t\t\ttopic 1 => 2\n",
      "doc: 2, token: trot\t\t\ttopic 0 => 1\n",
      "doc: 2, token: race\t\t\ttopic 2 => 1\n",
      "doc: 3, token: snail\t\t\ttopic 1 => 2\n",
      "doc: 3, token: the\t\t\ttopic 0 => 1\n",
      "doc: 3, token: turtle\t\t\ttopic 0 => 1\n",
      "doc: 4, token: time\t\t\ttopic 2 => 0\n",
      "doc: 4, token: travel\t\t\ttopic 1 => 2\n",
      "doc: 4, token: space\t\t\ttopic 0 => 2\n",
      "doc: 5, token: movie\t\t\ttopic 0 => 1\n",
      "doc: 5, token: on\t\t\ttopic 1 => 2\n",
      "doc: 5, token: thanksgiving\t\t\ttopic 0 => 2\n",
      "doc: 6, token: at\t\t\ttopic 2 => 1\n",
      "doc: 6, token: and\t\t\ttopic 0 => 1\n",
      "doc: 6, token: museum\t\t\ttopic 1 => 2\n",
      "doc: 6, token: is\t\t\ttopic 2 => 1\n",
      "doc: 7, token: star\t\t\ttopic 2 => 1\n"
     ]
    }
   ],
   "source": [
    "num_topics = 3\n",
    "alpha = 1\n",
    "eta = 0.001\n",
    "iterations = 10\n",
    "raw_docs=['eat turkey on turkey day holiday',\n",
    "      'i like to eat cake on holiday',\n",
    "      'turkey trot race on thanksgiving holiday',\n",
    "      'snail race the turtle',\n",
    "      'time travel space race',\n",
    "      'movie on thanksgiving',\n",
    "      'movie at air and space museum is cool movie',\n",
    "      'aspiring movie star']\n",
    "\n",
    "\n",
    "dt, wt, ta, inv_word_map = fit_topic_model(raw_docs,num_topics,iterations,eta,alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fbaeb3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic=pd.DataFrame(data=wt.transpose(),index=[inv_word_map[i] for i,j in enumerate(wt.transpose())],\n",
    "             columns=['topic_0','topic_1','topic_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a04bb0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['turkey', 'eat', 'like', 'thanksgiving', 'time']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic.sort_values(by='topic_0',ascending=False).head(5).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79b4f549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie', 'holiday', 'race', 'at', 'day']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic.sort_values(by='topic_1',ascending=False).head(5).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7beae973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on', 'race', 'space', 'travel', 'thanksgiving']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic.sort_values(by='topic_2',ascending=False).head(5).index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea27a2",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "1. With 3 topics and just 10 iterations, it looks like the model has already figured out a way to different certain topics\n",
    "2. Topic 0 is about 'eating turkey's' and 'thanksgiving'\n",
    "3. Topic 1 is about 'movies' and 'holidays'\n",
    "4. Topic 2 is about 'travel', 'race', but somehow also 'thanksgiving' - this one is little loose but perhaps more iterations can clean it up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beb6123",
   "metadata": {},
   "source": [
    "## Why Does this model work?\n",
    "\n",
    "To answer this, we will need to dig deep into the territory of Bayesian statistics to find some principled answers. A few questions we must ask first:\n",
    "\n",
    "1. What is the core hypothesis of an LDA topic model?\n",
    "2. Are there worse ways of designing the LDA model?\n",
    "3. Are there better ways of designing the LDA model?\n",
    "4. How do we develop an algorithm or classes of algorithms to train a model to discern topics in a set of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38436b83",
   "metadata": {},
   "source": [
    "$$ x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f8caf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
