{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcbff83",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation - MCMC implementation\n",
    "\n",
    "## Sources\n",
    "\n",
    "1. This notebook directly follows the on the same topic by Andrew Brooks. [link](http://brooksandrew.github.io/simpleblog/articles/latent-dirichlet-allocation-under-the-hood/)\n",
    "2. The Andrew Brooks tutorial strongly follows some of the original works of topic modeling by [Griffith et. al.](https://webfiles.uci.edu/msteyver/publications/Griffiths_Steyvers_Tenenbaum_2007.pdf)\n",
    "\n",
    "## Details\n",
    "\n",
    "While the original tutorial is in R, this code implements the same algorithm in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b702f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5642ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_topic_model(docs:list,num_topics:int):\n",
    "    \n",
    "    docs=[i.split() for i in docs]\n",
    "    word_map = {word:idx for idx,word in enumerate(list(set([i for j in docs for i in j])))}\n",
    "    docs=[[word_map[i] for i in j] for j in docs]\n",
    "    inv_word_map={k:v for v,k in word_map.items()}\n",
    "    \n",
    "    word_topic=np.zeros(shape=(num_topics,len(word_map)))\n",
    "    topic_assignment=[[0 for i in j] for j in docs]\n",
    "    #topic_assignment=[np.array(i) for i in np.array(topic_assignment,dtype=object)]\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "        for j in range(len(docs[i])):\n",
    "            topic_assignment[i][j]=np.random.randint(low=0,high=num_topics)\n",
    "            topic_index=topic_assignment[i][j]\n",
    "            word_index=docs[i][j]\n",
    "            word_topic[topic_index,word_index]+=1\n",
    "    doc_topic_count=np.zeros(shape=(len(docs),num_topics))\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "        for j in range(num_topics):\n",
    "            doc_topic_count[i,j]=len([k for k in topic_assignment[i] if k==j])\n",
    "            \n",
    "    return docs,word_map,inv_word_map,word_topic,topic_assignment,doc_topic_count\n",
    "\n",
    "\n",
    "def fit_topic_model(docs:list,num_topics:int,num_iterations:int,eta:float,alpha:float):\n",
    "    \n",
    "    docs,word_map,inv_word_map,word_topic,\\\n",
    "    topic_assignment,doc_topic_count = initialize_topic_model(docs,num_topics)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        for d in range(len(docs)):\n",
    "            for w in range(len(docs[d])):\n",
    "\n",
    "                topic_prev=topic_assignment[d][w]\n",
    "                word_id=docs[d][w]\n",
    "\n",
    "                doc_topic_count[d,topic_prev]-=1\n",
    "                word_topic[topic_prev,word_id]-=1\n",
    "\n",
    "                # Update topic assignment\n",
    "                denom_a = len(docs[d]) + num_topics * alpha\n",
    "                denom_b = word_topic.sum(axis=1) + len(word_map) * eta\n",
    "\n",
    "                update_probability=np.divide((word_topic[:,word_id] + eta),denom_b) \n",
    "                update_probability+= (doc_topic_count[d,:] + alpha)/denom_a\n",
    "                update_probability=update_probability/sum(update_probability)\n",
    "\n",
    "                # sample a word based on new update probability distribution\n",
    "                topic_new=np.random.choice(list(range(num_topics)),1,p=update_probability/sum(update_probability))[0]\n",
    "\n",
    "                topic_assignment[d][w]=topic_new\n",
    "                doc_topic_count[d,topic_new]+=1\n",
    "                word_topic[topic_new,word_id]+=1\n",
    "\n",
    "                if topic_prev!=topic_new:\n",
    "                    print('doc: {}, token: {}\\t\\t\\ttopic {} => {}'.format(d,inv_word_map[word_id],topic_prev,topic_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9f6ec852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 0, token: turkey\t\t\ttopic 0 => 1\n",
      "doc: 0, token: day\t\t\ttopic 0 => 1\n",
      "doc: 1, token: i\t\t\ttopic 0 => 1\n",
      "doc: 1, token: like\t\t\ttopic 1 => 0\n",
      "doc: 1, token: to\t\t\ttopic 0 => 1\n",
      "doc: 1, token: eat\t\t\ttopic 1 => 0\n",
      "doc: 1, token: cake\t\t\ttopic 1 => 0\n",
      "doc: 2, token: turkey\t\t\ttopic 1 => 0\n",
      "doc: 2, token: on\t\t\ttopic 0 => 1\n",
      "doc: 2, token: thanksgiving\t\t\ttopic 1 => 0\n",
      "doc: 2, token: holiday\t\t\ttopic 1 => 0\n",
      "doc: 3, token: turtle\t\t\ttopic 1 => 0\n",
      "doc: 4, token: space\t\t\ttopic 0 => 1\n",
      "doc: 4, token: race\t\t\ttopic 1 => 0\n",
      "doc: 5, token: on\t\t\ttopic 0 => 1\n",
      "doc: 5, token: thanksgiving\t\t\ttopic 1 => 0\n",
      "doc: 6, token: movie\t\t\ttopic 0 => 1\n",
      "doc: 6, token: and\t\t\ttopic 1 => 0\n",
      "doc: 6, token: museum\t\t\ttopic 1 => 0\n",
      "doc: 7, token: aspiring\t\t\ttopic 1 => 0\n",
      "doc: 0, token: turkey\t\t\ttopic 0 => 1\n",
      "doc: 0, token: turkey\t\t\ttopic 1 => 0\n",
      "doc: 0, token: day\t\t\ttopic 1 => 0\n",
      "doc: 0, token: holiday\t\t\ttopic 1 => 0\n",
      "doc: 1, token: i\t\t\ttopic 1 => 0\n",
      "doc: 1, token: like\t\t\ttopic 0 => 1\n",
      "doc: 2, token: on\t\t\ttopic 1 => 0\n",
      "doc: 3, token: race\t\t\ttopic 0 => 1\n",
      "doc: 3, token: the\t\t\ttopic 0 => 1\n",
      "doc: 4, token: time\t\t\ttopic 1 => 0\n",
      "doc: 4, token: travel\t\t\ttopic 1 => 0\n",
      "doc: 4, token: race\t\t\ttopic 0 => 1\n",
      "doc: 5, token: movie\t\t\ttopic 1 => 0\n",
      "doc: 5, token: on\t\t\ttopic 1 => 0\n",
      "doc: 6, token: movie\t\t\ttopic 1 => 0\n",
      "doc: 6, token: at\t\t\ttopic 1 => 0\n",
      "doc: 6, token: air\t\t\ttopic 1 => 0\n",
      "doc: 6, token: space\t\t\ttopic 1 => 0\n",
      "doc: 6, token: is\t\t\ttopic 0 => 1\n",
      "doc: 6, token: cool\t\t\ttopic 0 => 1\n",
      "doc: 6, token: movie\t\t\ttopic 1 => 0\n",
      "doc: 7, token: movie\t\t\ttopic 1 => 0\n"
     ]
    }
   ],
   "source": [
    "num_topics = 2\n",
    "alpha = 1\n",
    "eta = 0.001\n",
    "iterations = 2\n",
    "raw_docs=['eat turkey on turkey day holiday',\n",
    "      'i like to eat cake on holiday',\n",
    "      'turkey trot race on thanksgiving holiday',\n",
    "      'snail race the turtle',\n",
    "      'time travel space race',\n",
    "      'movie on thanksgiving',\n",
    "      'movie at air and space museum is cool movie',\n",
    "      'aspiring movie star']\n",
    "\n",
    "\n",
    "fit_topic_model(raw_docs,num_topics,iterations,eta,alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaeb3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
